# EX-02-Cross-Platform-Prompting-Evaluating-Diverse-Techniques-in-AI-Powered-Text-Summarization

## Aim:
Within a specific use case (e.g., summarizing text, answering technical questions), compare the performance, user experience, and response quality of prompting tools across these different AI platforms.
## AI TOOLS :
To evaluate the effectiveness of prompting tools across multiple AI
platforms—namely ChatGPT (OpenAI), Claude (Anthropic), Bard (Google),
Cohere Command, and Meta's AI models—focusing on diverse NLP use
cases such as summarization, technical Q&A, and creative content
generation.
## Overview of Prompting in AI :
Prompting tools have become pivotal in NLP applications, acting as the
interface through which users communicate complex tasks to AI systems.
These tools have evolved to support refined interaction, improved output
control, and diverse contextual understanding.
## Purpose of This Evaluation:

This study aims to assess and compare the capabilities, user experiences,
and output quality of several leading AI platforms. The comparison targets
tasks such as text summarization, code-related queries, and creative writing
to identify each tool's strengths and areas for growth.
## Evaluation Methodology
Use Case Selection:
Defined use cases:

Summarizing technical or legal content
Answering detailed technical/coding questions
Generating creative or narrative text
Comparison Criteria:
Response Quality: Accuracy, relevance, fluency, clarity
Performance: Response speed, reliability, and prompt consistency
User Experience: Interaction simplicity, customization, and control
Data Privacy: Handling of sensitive information and transparency
Prompt Complexity Handling: From simple queries to context-rich
instructions
Customization & Fine-Tuning: Flexibility through prompt design or settings
like temperature, system instructions
## Use Case 1: Summarizing Complex Text
Prompt Setup:
Each platform was tested using a dense, domain-specific document (e.g.,
legal policy or scientific article) and asked to produce summaries of varying
lengths and styles.
## Platform-Specific Observations:
ChatGPT: Offers structured summaries and strong coherence, but may
simplify complex jargon.
Claude: Notable for context retention and brevity, with clear paragraphing.
Bard: Context-rich summaries, often integrating external data, though

sometimes verbose.
## Comparative Insights:
Readability & Precision: Claude and ChatGPT lead in clarity; Bard shines in
information depth.
Terminology Handling: Claude showed stronger retention of domain-
specific language.
Summary Length Control & Rephrasing: ChatGPT and Claude handle these
well; Bard less so.
Use Case 2: Answering Technical Questions
## Prompt Setup:
A set of prompts required platforms to write or debug code, explain
algorithms, or troubleshoot technical errors across multiple programming
languages.
## Platform-Specific Observations:
ChatGPT: Excellent coding support, logical explanations, and debug
assistance.
Claude: Superior at reasoning over multiple steps; maintains long-context
queries well.
Bard: Good with basic queries but occasionally outdated or imprecise.
## Comparative Insights
Code Accuracy & Reasoning: ChatGPT and Claude perform best.
Error Handling: Claude detects and explains bugs more reliably.
Language Support: ChatGPT handles language transitions smoothly.

Latest Tech Knowledge: ChatGPT stays most current; Bard is occasionally
behind.
5. Use Case 3: Text Generation & Creative Tasks
## Prompt Setup:
Used for storytelling, rewriting, brainstorming, and generating content with
emotional or humorous tones.
## Platform-Specific Observations:
ChatGPT: Balances creativity and logic; highly adaptable in tone.
Claude: Strong storytelling flow with thoughtful development.
Bard: Generates unique ideas but less refined narrative structure.
## Comparative Insights
Originality & Engagement: Claude is more imaginative; ChatGPT is more
structured.
Tone/Genre Flexibility: ChatGPT leads with seamless transitions.
Handling Sensitive Content: Claude is cautious but balanced; ChatGPT flags
content appropriately.
## Performance and Technical Metrics
Speed: Bard is typically fastest; ChatGPT is consistent but slower with long
prompts.
Context Retention: Claude excels in multi-turn, long-context chats.
Scalability & Load: All platforms handle load well, though Bard shows slight
delays under strain.
5
Error Handling: ChatGPT recovers gracefully from incomplete inputs;
Claude prompts for clarification.
## User Experience Overview
Ease of Use: ChatGPT and Bard offer the most intuitive interfaces.
Control Mechanisms: ChatGPT allows detailed prompt tuning (e.g., system
roles, temperature).
User Adaptability: Claude adjusts mid-conversation better than others.
Beginner Friendliness: Bard and ChatGPT are easiest for new users.
Enterprise Features: ChatGPT provides enterprise APIs and team tools.
Feedback Integration: ChatGPT and Claude actively incorporate user
feedback to improve responses.
## Conclusion & Recommendations
Summary of Key Findings:
Best for Summarization: Claude, followed by ChatGPT.
Best for Technical Tasks: ChatGPT, then Claude.
Best for Creative Work: Claude leads in creativity; ChatGPT in structure and
coherence.

## Industry-Specific Suggestions
Healthcare & Legal: Claude (better jargon handling)
Software & Engineering: ChatGPT (technical accuracy)
Marketing & Media: Claude or Bard (for creative ideation)
## Limitations & Workarounds
Bard may lack depth; pairing with ChatGPT enhances detail.
Claude struggles with newer APIs; ChatGPT compensates.
## Cost Consideration
⦁ ChatGPT and Claude offer freemium and enterprise models.
⦁ Bard currently remains free but lacks robust customization.
⦁ Value depends on usage intensity and precision required.
## Future Outlook
⦁ Long-term memory
⦁ Multi-modal prompting (voice/image)
⦁ Better feedback loops
⦁ Real-time conversational agents
## Result:
Thus, the evaluation of 2024 prompting tools across leading AI platforms-
ChatGpt , Claude , Bard ,Cohere Command, and Meta’s based models has
been analysed.
